---
title: "studio_graph2に数学の教科書をオススメしよう！"
date: 2018-01-19 14:29:04
---

twitter上の有名な院生アカウントで、なんでもディープラーニングでなんとかする事に定評のあるstudio_graph2氏が、ちょっと数学でもやってみようかな、と思っていると言っていたのでディープラーニングでなんとかする人向けの数学のオススメをしてみようのコーナー。

## 前提

- 微積、線形代数に加えて、フーリエ解析とベクトル解析と複素関数論と微分方程式をちょっとかじったくらい（本人談）
-  普段からChainerとかLightGBMとかでブイブイ言わせてて論文とかも普通に読んでいる優秀な修士くらいをイメージ
- なるべく数学なしで済ますにはどうしたらいいか？という話じゃなくて、ちゃんとやるならどこからやっていくか？という話。
- 当然最終的には私よりもちゃんとやる子になる予定なので、私に出来るのは最初の方だけ。

ではいってみよう。



## 入門数理統計学 (ホーエル)

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4563008281&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

まずは確率のちゃんとした入門から。最終的には我々はベイズでやっていく訳だけど、基礎の所はちゃんと頻度主義でやっておく方が良いと自分は思う。
とはいえやはり頻度主義的な分析の重要度はDeep Learningでは低いので、この位一冊やっておけば十分かな、と思う。

この本は測度論を使わない割には、ちゃんと積率母関数などを使って中心極限定理などの証明が入っていて、厳密さとわかりやすさのバランスが良い。

昔はこの位知っておけば専門外の人は十分という教科書だったのだが、今となっては最初の入門。時代も変わったね。

## ルベーグ積分から確率論 （共立講座 21世紀の数学）

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4320015622&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

残念な事に我々は測度論が必要な時代を生きているので、諦めて測度論をやる必要はある。
だが、確率論以外では我々は使わない。
だから確率論で必要最小限な測度論だけでお茶を濁したい。

その目的の本は幾つかあるけれど、この本が最も初心者向けでありながら必要な事が網羅されていると思う。
このシリーズは表面上は凄いガチ系の本に見えて近寄りたくない気がするけれど、この巻に限っていえばかなり初心者向けと言える。
といっても測度論をやる人にしては初心者、というだけなので、やはり結構ガチな数学である。
完備とか出てくるので、その辺はもう少し優しい本でどこかで慣れておく方が良いかもしれない（集合論で出て来る）

これでは測度論やルベーグ積分は不足する内容も多いが、幸い機械学習やDeep Learningでそれらの不足する内容を要求される事は無い（今の所）

この本は確率空間的な話がバリバリ出てくるので、その辺の訓練にもちょうど良い。
Deep Learningの論文はだいたい確率空間の言葉で定式化されるので、言葉に慣れておく必要はある。
この本は内容が簡単な割にはちゃんと確率空間上の話なので、その練習にちょうど良い。

志の低い事を言えば、流行りの論文を理解していく事を目的とするなら、
この本で扱っている証明や定理などまで理解している必要は無い。
論文を理解するだけなら、シグマ集合体、ボレル集合体、ルベーグ測度、a.e.収束、などの基礎的な概念の定義だけ知っておいて、それらの定義をした上で展開される議論のあらすじくらいを知っておく程度で我々が必要な程度の理解は出来ると思う。
そういう意味で定義知っておくだけ、という向きにもやはり手元に置いておくと便利な本だ。

なお、そういうのが登場するような論文を「書く」側に回るなら、基礎的な定理を理解して測度論の基礎をちゃんと修めている必要はあるだろう。
自分は一応証明などは一度理解したつもりだが、もう忘れているので今はそのレベルには居ない。
こんな大人になってはいけない。

## カルマンフィルタや確率過程は要るか？（私は今は要らないと思う）

さて、次は確率過程に進むのが5年前の流れだが、現代はDeep Learning全盛なので、とりあえず確率過程には進まなくて良いんじゃないか、というのが自分の意見。
必要になったらやったらいいと思うが、ここはここで長い道のりがある割には、自分は実務で使った事無い。

一応簡単に、どういう時に要るかという話をすると、

1. データが少ない時系列分析には要る
2. 動きが確率的な時系列分析をする時（株価などのギザギザ度がやばい奴）をやる時には要る

なんとなくパーソナライズしたリコメンドとかで使いそうな気がするんだが、あんま使ってるのは見た事ない。

仮想通貨で大儲けにょ、とか思っても要るとは思うが、まぁそういうのはいいでしょう。

そのうちロボットとかのセンサー系で2をバリバリやらないといけない日は来るかもしれないが、
昨今は割とデータ量でぶん殴れば大丈夫だったりするので、確率過程はDeep Learningでなんとかする世代は後回しでもいいと思う（のでここでは紹介しない）

ただ、最近はEdwardとか流行りそうなので、ここらへんが必修にな時代が来てしまうかもしれない。そうしたら諦めてやろう。

## 基礎からのベイズ統計学

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4254122128&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

さて、我々機械学習屋は基本的にはベイズでやっていく生き物である。
という事でベイズ統計を学ぶ必要がある。しかもかなり本気で。

さらに、Deep Learning系の論文を読む上で一番追うのが辛いのは、期待値計算の所だ。
だから期待値計算の追い方をどこか実際の論文よりはぬるい所で訓練しておくのがゆとりの生き様って奴だと思う。
そこでこの本だ。

この本はMCMCの本なのだが、最初の方はベイズ統計の入門になっていて、これが良く書けている。
例題も豊富で解答もしっかり書いてあるので、受験参考書で慣れた人にはありがたい。
しかも非常に平易に書いてある。

MCMCは要るのか？ってのは結構微妙で、少なくとも私は実務で使った事無い。
この辺は確率過程の所の自分のコメントとだいたいかぶる。

ただ、MCMCは生成モデル全盛の現在においては、サンプリング周辺をしっかり学ぶ手頃な問題なので、その意味でGANの前の準備体操としてやっておくのに凄く向いている。

本書はプログラムのコード的なのを意識した内容となっているので、コードを書きながら読めるのも我々にはありがたい。
自分はPythonで再実装しながら読んだ。
でもなぜかPyStanはうまくインストール出来なかったので、Stanが必要になった後はRStanでやった。

また、先程も触れたがMCMCは結構式が抽象的になるので、期待値計算の練習に良い。
説明もしっかりしているし、抽象的とはいえ所詮GPU以前の時代でも計算出来たような世界なので、
現在のDeep Learning物の期待値計算よりは大分楽だ。
期待値計算はここらへんで練習しておくのが、実際の論文読む手頃な練習になると思う。

## 雑談的な補足

ここまで終わってれば、機械学習の論文を読むという点では私と同程度の理解となります。
私の同程度の理解では読めない論文もちょこちょこありますが、Deep Learningに限って言えばそんな多くは無い。
書く方に回る事を考えるともう一段上の数学力が要ると思いますが、自分はそこのステージには居ないので、ここから先は別の人に聞いてください。

以下、そのほか雑談的な話を。

### PRML

まぁこれは私が何か言う必要は無いのでリンクも貼らんです。

ベイズの計算練習としてはやはり現在でも良い本です。
式変形も論文よりは大分丁寧なので、練習の題材としては良いと思う。
特に期待値計算に関しては論文で必要な程度の訓練は積めると思う。

ただちょっと題材が古いので、こう、キラキラ感には欠けるね。

### 変分法が無いんだが？

我々は残念な事に変分ベイズが必要な時代を生きている気がする。
変分ベイズ自体はPRMLでも読め、という事だが、変分法はやはり別の所で学んでいる必要がある。

でも変分法の良い教科書とかは知りません。自分は理論物理の出身なので、昔ある程度やってたのでなんとかなってしまいました。


### 関数解析が無いんだが？


我々は残念な事に関数解析が必須な時代を生きているようです。
これは多分変分ベイズより必須。ぐぬぬ。

それはGANが関数空間の上での近似を扱うからです。
でもゆとりな自分は関数解析とか歯が立たないのでGANとか分からない系男子です。
マスターして教えてください。

### 実際どこまで要るの？

ここに挙げた物を全部まったくやらなくても、たぶんコードを書くくらいは出来る場合が多いと思う。
論文を読んで、重要なコアな所の変形とかは良く分からないが、結論とそのコードへの落とし方くらいは読み取れます、という程度なら、必要な数学レベルはぐっと落ちる。
特に昨今の流行りの論文を追うくらいなら、誰かがコード書いててくれている事も多いので、カンニングも出来るしね。

一方でそれだときつい事も、やはり結構あります。
論文書く場合はそれが顕著ですが、書かなくても結構辛い論文に当たる事はあります。
特に自分のテーマが少し流行りから外れると、一気に解説とかサンプル実装がなくなったりするし、
少し古い論文との比較実験をしなくちゃいけない時も、やはり一段上の論文読解能力を要求される。
その位だと、やっぱりここで挙げた本くらいは要る気がする。

逆にここで挙げた位をやっておけば、Deep Learning系の仕事なら十分一通りの事は出来るかなぁ、という気がする。むしろ数学はプログラム書ける中では出来る方になるんじゃないでしょうか。
理解出来ない論文も幾つかは出て来るけれど、その位は数学詳しい同僚なり先輩なりに教えてもらっても良いレベルと思います。

といってもその辺の認識はあくまで私の場所から見える風景なので、開発室グラフ先生におかれましては、そんな低い所を目指さずにもっとずーっと先まで行って私に説教したりしてくだされ。

### 数学が苦手な人への注意

ここに挙げたのは結構要求するスタート地点が高いです。
決して数学分からないけど機械学習とかやってみたい、という人向けの話ではありません。
という事でそういう人はここの内容読んで絶望したりしなくて良くて、とりあえず出来る範囲で触る所から始めたらいいんじゃないでしょうか。
