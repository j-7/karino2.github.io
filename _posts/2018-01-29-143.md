---
title: "深層学習による自然言語処理"
date: 2018-01-29 11:20:15
---

詳解ディープラーニングがいまいちだったので、もうちょっと別の本も読むか、と思って買った本。
さらさらっと読んで見る予定。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4061529242&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

一章、二章はあまり新しい事も無いのでさらっと流す。

3章の言語モデルは、NN以前の機械翻訳の勉強をした時にやったが、良い機会なのでそこそこ真面目に読み直しておく。

3.2.5でperplexityの定義を確認しておく。
このスコアで良く比較してるよね。

3.3は分散表現としてembeddingsの歴史とか解説してるが、これはなかなか詳しい。良く書けているなぁ。

### 3.3.4のnegative samplingの説明が分からん

式を見ても良く分からない。
このへんは[昔](http://jbbs.shitaraba.net/bbs/read.cgi/study/12706/1489317317/)やったんだが、もう忘れた。
4.3.4で詳しくやると言っているのでそちらを見てみる。

すると4.3.3の続きっぽいので4.3.3を軽く眺めると、NCEの話。ああ、分配関数をノイズとのロジスティック回帰で求める、みたいな話か。なんかあったな。

式を眺めただけだがなんとなく思い出したのでnegative samplingを見る。

式自体はnegativeなサンプルとpositiveなサンプルをロジスティック回帰で識別器を作っているように見える。
問題はこの識別器を何に使うかが良く分からんことだな。
3.3.4に戻ってみよう。

3.24式は、ロジスティック回帰のコスト関数になってるな。
で、このファイが学習対象になるんだよな。
ファイは3.18式で、まさにembeddingsを計算している。

つまり、このロジスティック回帰の結果としてembeddingsが得られるのか。

それの確率的な解釈とかは4章でやれば良かろう。今は3.24と3.25を理解しただけで良しとする。

### 3.4.2 系列変換モデルのモデル構造

seq2seqの類のモデルの、一般的な数式の記述。
このレベルだと簡単だよな。

こんなに丁寧に書いてある解説は初めて見た。

もっと短く書けると思うけれど、長い記述を飛ばしながら読む方が、簡潔だけど分からない行間をウンウンうなりながら埋めるよりはずっと楽なので、なかなか好印象。

### 三章を読み終わっての雑感

読み物的な物が挟まってるのと、
記述自体が概要程度に留まってるおかげで、数式が多く見える割にはサラサラ読める。

文章で説明する所と数式で説明する所のバランスが良くて、
数式を細かく見ないでも何を言いたいのか分かる感じで書かれている。
これは書いた人が良く理解している、という事なのかもしれない。

その代わり読んでて何も引っかからないので、自分は多分あんまり分かってないんだろうなぁ、という気もしてくる。
どうしたらいいのかなぁ。実際に実装してみたらいいのか？

結構短い中に必要な事がまとまってるので、一度理解したあとに手元に置いとくのには良い本な気がした。
すぐ読み直せるし。

### 4.1のattentionが良く書けている

さらっと書かれている割には、ハードなattentionの期待値計算の式変形は必要な途中式がちゃんと書かれていて、分かる人には簡単に読み進められるようになっている。
地味に著者は良い仕事をしているなぁ。

一方で、こういう本はあんま売れないかもなぁ、という気もする。
数式をちゃんと理解出来るようにはなっても、とりあえずコピペでなんか動かした気になるコードとかが無いので、
ちゃんと読まないと分かった気になれない。
コードがあると、説明は意味不明でもとりあえず動かす事は出来て、それをやると何か分かった気はする（実際何か分かる事もある）。  
最低限の所をちゃんと分かる為に必要な事を丁寧にやる、って、あんまりウケが良く無いよなぁ。もっと流行りの複雑な奴を、分かってるフリして書く方がウケる傾向にある気がする。

でもそういうのは不健全だよなぁ。
ろくでも無い本とか記事をすぐシェアして、こういう地味だけどしっかりした本とかには全然反応しないのは良くない。

という事でこの本は素晴らしいのでこの分野に興味有る人は読んでみて良いんじゃないかと思う。
これだけで全部ちゃんと分かる、という感じでは無いが、様々な要素技術の関係は理解出来て、必要な所だけは元論文を読んでいく、という基礎にはなってると思う。

### 4.2のmemory  network

4.2.3までは結構真面目に元論文を読んだ事があるので、このくらいの記述で良い復習になった。

4.2.4のDMNは初見なのでこの説明では全然分からないが、そういうのがあるのね、という知識くらいは得られた。
この本の立ち位置はこういう感じだよね。

### 2.2式のyチルダってなんだろう？

4.3.1てソフトマックスの分配関数の近似の所で、yチルダというのが出ているが定義が分からん。

見直してみると2.2式こクロスエントロピーの分母に、yチルダという文字が出てくるが、この定義が見つからない。
これは何か？

キャピタルYの分布に従う変数というだけっぽいが、このチルダの意味はなんなのかなぁ。
チルダ無しは教師データという意味という事かなぁ。

### 4章の分配関数周辺いろいろ

NCEやnegative samplingは以前やった事があるので、この説明をみてたら割と思い出した。
4.3.5のブラックアウトだけ初見だったので、これは説明を軽く読むだけじゃ良く分からない。
まぁあんま興味も無いので名前だけ知っておけば十分だろう。

4.3.6の階層型softmaxも初見だが、こちらはアイデアは単純なので概要はこれでも十分分かる。

## 5章、様々な応用

5章は深層学習を用いた実際の応用例を見ていく章とのこと。
問題設定とかの解説が割としっかり書かれていて、実際のモデルについては幾つか代表的な物の名前を挙げた上で、一つを詳しく見ていく、という構成。

どういう分野があって、どんな事をやってるのかをざっと把握するには良い。

### 5.1 機械翻訳

最初はOpen NMTというattentionつきのseq2peqのシステムの構成を擬似コードなどを交えて見ていく、というもの。

実際に実装出来るくらいの理解には到達しないが、どんな物かは分かる、くらいには説明されている。

2017/1に書かれているとの事なので、話題もなかなか現代的。

### 5.2 文書要約

この辺はLDAのトピックモデルくらいで自分の知識は止まってたので、雑学として読んだ。
5.2.3の細かい式はあんまり真面目に追ってないで、図を見て分かった気になって進む。

紹介されている例はRNNですら無くかなり原始的なモデルなので、そんな難しくは無さそう。

この本の解説を信じるなら、文書要約はまだまだ難しそうだな。教師データをどうするか、というあたりが。教師なしの方向を模索するのが良いのかもしれないけれど。  
この辺はもうちょっと進んでる印象だったので、まぁまぁ意外だ。

### 5.3から先

5.3の対話モデルは概要のみか

この辺は読んでも具体的な事は全然分からず、こんなアイデアもあるのか、と概観する程度だなぁ。

評価の難しさとかを見てると、まだまだ実用にはならなさそうな印象を受ける。

5.4は質問応答。
これに至っては、問題設定とか手法の名前くらいで、実際の実装の話はあまり出てこない。
以前memory networkの元論文とかでこの辺やってたのでもともと多少の知識はるが。
これはトピックが存在する、程度の情報しか得られないが、そういう意図のセクションなのだろうな。


### 5章雑感

前半の翻訳とかがメインで、後半はこんなトピックもあるんだね〜、その辺が難しさなのね〜、くらいで読み流す構成となっている。
この本を読む時の一番の関心も翻訳だと思うので、これは悪くないと思う。

翻訳の解説は結構詳しい。これだけでなんとなく雰囲気は伝わるし、元論文を辿っていった人にも補助資料として役に立つと思う。
これだけでちゃんと分かるのを期待すると期待はずれだが、論文を読み進める前の入り口としての役割はちゃんと果たせているんじゃないか。

後半のトピックの紹介程度の話も割と良く書けているので、「こんなのあるだけ無駄」とは思わなかった。
バッサリ詳細を省く事で、結構無駄無く目的を達成していて、結構しっかり考えて書かれた本だな、と好印象。

ただ、この章で自然言語処理の主要な話は終わってしまう事を思うと、本全体としてはちょっと薄味だな、と感じてしまう。
まぁもともとそういうシリーズという気はする。

詳解ディープラーニングが意味不明な説明が多かったのとは好対照で、分かりにくいところはまったく無い。難そうな所は省かれていたりする事もあるが、その辺は意図を持ってやってるのが伝わってくるので、不満は無い。  
深層学習を使ったNLPの入り口として良い本だね。

## 6章、学習回りのテクニック

L2正則化、early stopping、transfer learningなどの話。
この辺は別に自然言語処理関係ないので、知らん事もそんな無いなぁ。

RNNの初期値とかは知らない事もあるので、目を通すくらいは通しておく。 

6章はだいたい知ってる人が、抜けを確認する為の読み物、くらいの印象。
