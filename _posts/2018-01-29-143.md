---
title: "深層学習による自然言語処理"
date: 2018-01-29 11:20:15
---

詳解ディープラーニングがいまいちだったので、もうちょっと別の本も読むか、と思って買った本。
さらさらっと読んで見る予定。

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=qf_sp_asin_til&t=karino203-22&m=amazon&o=9&p=8&l=as1&IS1=1&detail=1&asins=4061529242&bc1=ffffff&lt1=_top&fc1=333333&lc1=0066c0&bg1=ffffff&f=ifr"> </iframe>

一章、二章はあまり新しい事も無いのでさらっと流す。

3章の言語モデルは、NN以前の機械翻訳の勉強をした時にやったが、良い機会なのでそこそこ真面目に読み直しておく。

3.2.5でperplexityの定義を確認しておく。
このスコアで良く比較してるよね。

3.3は分散表現としてembeddingsの歴史とか解説してるが、これはなかなか詳しい。良く書けているなぁ。

### 3.3.4のnegative samplingの説明が分からん

式を見ても良く分からない。
このへんは[昔](http://jbbs.shitaraba.net/bbs/read.cgi/study/12706/1489317317/)やったんだが、もう忘れた。
4.3.4で詳しくやると言っているのでそちらを見てみる。

すると4.3.3の続きっぽいので4.3.3を軽く眺めると、NCEの話。ああ、分配関数をノイズとのロジスティック回帰で求める、みたいな話か。なんかあったな。

式を眺めただけだがなんとなく思い出したのでnegative samplingを見る。

式自体はnegativeなサンプルとpositiveなサンプルをロジスティック回帰で識別器を作っているように見える。
問題はこの識別器を何に使うかが良く分からんことだな。
3.3.4に戻ってみよう。

3.24式は、ロジスティック回帰のコスト関数になってるな。
で、このファイが学習対象になるんだよな。
ファイは3.18式で、まさにembeddingsを計算している。

つまり、このロジスティック回帰の結果としてembeddingsが得られるのか。

それの確率的な解釈とかは4章でやれば良かろう。今は3.24と3.25を理解しただけで良しとする。
